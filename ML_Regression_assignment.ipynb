{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsx/YuipisrzJljxqQyHsS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishekgutte200/Python_programs/blob/main/ML_Regression_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. What is Simple Linear Regression (SLR)?\n",
        "Simple Linear Regression (SLR) is a statistical method that models the relationship between a dependent variable (Y) and a single independent variable (X) by fitting a linear equation to the observed data.\n",
        "\n",
        "### 2. Key assumptions of Simple Linear Regression:\n",
        "   - **Linearity**: There is a linear relationship between the dependent and independent variable.\n",
        "   - **Independence**: Observations are independent of each other.\n",
        "   - **Homoscedasticity**: Constant variance of errors (residuals).\n",
        "   - **Normality**: The errors are normally distributed.\n",
        "\n",
        "### 3. What does the coefficient \\( C \\) represent in the equation?\n",
        "   - The **coefficient** \\( C \\) represents the slope of the regression line in the equation of the form:\n",
        "     \\[\n",
        "     Y = C + bX\n",
        "     \\]\n",
        "   - This means that for each unit change in \\( X \\), the dependent variable \\( Y \\) will change by \\( C \\) units.\n",
        "\n",
        "### 4. What does the intercept \\( c \\) represent in the equation?\n",
        "   - The **intercept** \\( c \\) represents the value of \\( Y \\) when \\( X = 0 \\). It is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "### 5. How do we calculate the standard error in Simple Linear Regression?\n",
        "   - The standard error can be calculated by taking the square root of the mean squared residual error:\n",
        "     \\[\n",
        "     SE = \\sqrt{\\frac{\\sum (Y_i - \\hat{Y_i})^2}{n - 2}}\n",
        "     \\]\n",
        "   where \\( Y_i \\) are the observed values and \\( \\hat{Y_i} \\) are the predicted values.\n",
        "\n",
        "### 6. What is the purpose of the Least Squares method in Simple Linear Regression?\n",
        "   - The **Least Squares method** minimizes the sum of the squared differences between the observed values and the predicted values, providing the best fit for the data.\n",
        "\n",
        "### 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "   - **R²** indicates the proportion of variance in the dependent variable that is explained by the independent variable. It ranges from 0 to 1, with higher values indicating a better fit.\n",
        "\n",
        "### 8. What is Multiple Linear Regression?\n",
        "   - **Multiple Linear Regression (MLR)** is an extension of simple linear regression that models the relationship between two or more independent variables and a dependent variable.\n",
        "\n",
        "### 9. How is the coefficient in Multiple Linear Regression interpreted?\n",
        "   - Each **coefficient** in MLR represents the change in the dependent variable for a one-unit change in the corresponding independent variable, assuming all other variables are held constant.\n",
        "\n",
        "### 10. What are the key assumptions of Multiple Linear Regression?\n",
        "   - **Linearity**: The relationship between the dependent variable and independent variables is linear.\n",
        "   - **Independence**: Observations are independent of each other.\n",
        "   - **Multicollinearity**: Independent variables should not be too highly correlated with each other.\n",
        "   - **Homoscedasticity**: Constant variance of errors.\n",
        "   - **Normality**: The errors are normally distributed.\n",
        "\n",
        "### 11. What is Multicollinearity in Multiple Linear Regression?\n",
        "   - **Multicollinearity** occurs when two or more independent variables are highly correlated, making it difficult to assess the individual effect of each variable.\n",
        "\n",
        "### 12. How do we deal with Multicollinearity in Multiple Linear Regression?\n",
        "   - Use techniques like **Variance Inflation Factor (VIF)** to check for multicollinearity and consider **removing highly correlated variables** or using **principal component analysis (PCA)**.\n",
        "\n",
        "### 13. How does interaction between predictors affect the results of a Multiple Linear Regression Model?\n",
        "   - Interaction terms in MLR capture the combined effect of two or more predictors on the dependent variable, which may differ from their individual effects.\n",
        "\n",
        "### 14. How can you improve a Multiple Linear Regression model?\n",
        "   - **Feature selection**: Removing irrelevant features.\n",
        "   - **Regularization**: Using techniques like Ridge or Lasso regression.\n",
        "   - **Interaction terms**: Including relevant interaction terms if needed.\n",
        "\n",
        "### 15. What is Adjusted R² and why is it important?\n",
        "   - **Adjusted R²** adjusts R² for the number of predictors in the model. It is used to compare models with different numbers of predictors, as it penalizes the addition of irrelevant variables.\n",
        "\n",
        "### 16. How is residual analysis used in regression?\n",
        "   - **Residual analysis** involves analyzing the residuals (differences between observed and predicted values) to check if the assumptions of the regression model hold true.\n",
        "\n",
        "### 17. What are the effects of heteroscedasticity in regression?\n",
        "   - **Heteroscedasticity** means that the variance of residuals is not constant, which can lead to biased estimates of standard errors and thus affect hypothesis testing.\n",
        "\n",
        "### 18. What is the significance of polynomial regression?\n",
        "   - **Polynomial regression** is used when the relationship between the independent and dependent variables is nonlinear. It includes polynomial terms (e.g., \\( X^2, X^3 \\)) to model the curvature.\n",
        "\n",
        "### 19. How does Polynomial Regression differ from Simple Linear Regression?\n",
        "   - Polynomial regression can model nonlinear relationships, while simple linear regression can only capture linear relationships.\n",
        "\n",
        "### 20. When is Polynomial Regression used?\n",
        "   - Polynomial regression is used when the data exhibits a curvilinear pattern, rather than a straight-line trend.\n",
        "\n",
        "### 21. What is the general equation for Polynomial Regression?\n",
        "   - The equation for a second-degree polynomial regression would be:\n",
        "     \\[\n",
        "     Y = b_0 + b_1X + b_2X^2 + \\epsilon\n",
        "     \\]\n",
        "   - Higher-degree polynomials can also be used, depending on the data.\n",
        "\n",
        "### 22. What is the main difference between Linear Regression and Polynomial Regression in terms of model complexity?\n",
        "   - Polynomial regression introduces higher-degree terms, making the model more complex and capable of capturing nonlinear trends, whereas linear regression models a straight-line relationship.\n",
        "\n",
        "### 23. What are the main considerations when using polynomial regression?\n",
        "   - **Overfitting**: High-degree polynomials can overfit the data, capturing noise rather than the underlying trend.\n",
        "   - **Multicollinearity**: Higher-degree polynomials can introduce collinearity between predictors.\n",
        "\n",
        "### 24. How does the inclusion of higher-degree terms affect polynomial regression?\n",
        "   - Higher-degree terms allow the model to capture more complex, curved patterns, but can also lead to overfitting and instability.\n",
        "\n",
        "### 25. What is the general procedure to transform categorical variables for use in regression models?\n",
        "   - **One-hot encoding** is commonly used to transform categorical variables into binary variables for use in regression models.\n",
        "\n",
        "### 26. How does interaction between categorical and continuous variables work in Multiple Regression?\n",
        "   - Interaction terms between categorical and continuous variables allow you to assess how the relationship between the continuous variable and the dependent variable changes across different levels of the categorical variable.\n",
        "\n",
        "### 27. How does collinearity affect a Multiple Regression model?\n",
        "   - **Collinearity** makes it difficult to determine the individual effect of each predictor on the dependent variable and may inflate standard errors, making statistical tests unreliable.\n",
        "\n",
        "### 28. How can you interpret interaction terms in Multiple Linear Regression?\n",
        "   - Interaction terms indicate that the effect of one variable on the dependent variable depends on the level of another variable.\n",
        "\n",
        "### 29. How do you interpret Adjusted R²?\n",
        "   - Adjusted R² provides a more accurate measure of model fit by accounting for the number of predictors. It penalizes the model for including irrelevant variables, ensuring a better comparison across models.\n",
        "\n",
        "### 30. How do you deal with multicollinearity in a Multiple Linear Regression?\n",
        "   - Use **Variance Inflation Factor (VIF)** to check for multicollinearity and remove highly correlated variables, or apply techniques like **Ridge** or **Lasso** regression for regularization.\n",
        "\n"
      ],
      "metadata": {
        "id": "XHTTNHBUqTYj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXppp3b6rKIa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}